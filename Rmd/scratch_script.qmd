---
title: "A script for test"
author: "Sebastian Di Geronimo"
date: "2023-10-07"
format: html
---

# 1.0 ---- Summary of Document ----
- 3.0: Illustration of projecting data points


# 2.0 ---- Setup ----

## 2.1 Load Libraries
```{r setup, include=FALSE}
if (!nzchar(system.file(package = "librarian"))) 
  install.packages("librarian")

librarian::shelf(
  quiet = TRUE,
  librarian, conflicted, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
  forcats, lubridate, glue, fs, magrittr, here,
  
  # additional
  
)

conflicts_prefer(
  dplyr::filter(), 
  dplyr::select()
  )
```

# 3.0 ---- Illustration of projecting data points ----
From: 
\cite{shaliziAdvancedDataAnalysis2021}
“Figure 15.1” ([Shalizi, 2021, p. 321](https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/))

Illustration of projecting data points $ \vec x $  (black dots) on to an 
arbitrary line through the space (blue, dashed), represented by a unit 
vector $ \vec w $ along the line (also blue but solid). The blue dots are the 
projections on to the blue line,$ (\vec x~\dot~\vec w)\dot~\vec w $ ; the gray 
lines are the vector residuals,$ \vec x~-~(\vec x~\dot~\vec w)\dot~\vec w $. 
These are not the residuals from regressing one of the components of the data 
vector on the other.


```{r section-3.0}
demo.theta <- runif(10, min    = 0, max = pi/2) 
demo.x <- cbind(cos(demo.theta), sin(demo.theta)) 
demo.x <- scale(demo.x, center = TRUE, scale = FALSE) 

demo.w <- c(cos(-3 * pi/8), sin(-3 * pi/8)) 

# x * y * x
# [10 x 2] * [2 x 1] * [10 x 2]
projection <- demo.x %*% demo.w %*% demo.w 

{
plot(demo.x, 
     xlab = expression(x^1), 
     ylab = expression(x^2), 
     xlim = c(-1, 1), 
     ylim = c(-1, 1))
arrows(0, 0, demo.w[1], demo.w[2], col = "blue") 
text(demo.w[1], demo.w[2], pos = 4, labels = expression(w)) 
abline(0, b = demo.w[2]/demo.w[1], col = "blue", lty = "dashed") 

points(projections, pch = 16, col = "blue") 
segments(x0 = demo.x[, 1], y0 = demo.x[, 2], x1 = projections[, 1], y1 = projections[, 2], col = "grey")
}
```
# 4.0 ---- Trelliscope Test ----

from [trelliscope](https://github.com/trelliscope/trelliscope)
web: [trelliscope reference manual](https://trelliscope.org/trelliscope/)

Supersedes: [trelliscopejh](https://github.com/hafen/trelliscopejs)
  - although has a nice looking interface: 
    - [JS version](https://hafen.github.io/trelliscopejs/articles/trelliscopejs.html#-ggplot2-interface-facet-trelliscope)


Not to be confused with: [trelliscope](https://github.com/delta-rho/trelliscope)

```{r trelliscope}
shelf(trelliscope)

szn <- c("Winter", "Spring", "Summer", "Autumn")

# rstudioapi::selectDirectory()
file_dir <- here("..", "new_MBON_phytoplankton_pigments", "data", "plots", "map", "ratio")

test <-
  file_dir %>%
  dir_ls() %>%
  tibble(img_src = .) %>%
  # slice(1:2) %>%
  mutate(
    .before = 1,
    id      = row_number(), 
    base    = basename(img_src),
    base    = path_ext_remove(base),
    season  = str_extract(base, ".*_ratio_(.*)_20", group = 1),
    season  = str_to_title(season),
    season  = fct(season, szn),
    pigment = str_extract(base, "mbon_pigm_(.*)_ratio", group = 1),
    ) %>% 
  as_trelliscope_df(
    name = "Display Pigments", 
    key_sig = "img_src",
    key_cols = c("season", "pigment"),
    tags = "base_name"
    ) %>%
    set_default_layout(ncol = 2, page = 2) %>%
    add_inputs(
      input_checkbox(
        name = "Checkbox Input",
        label = "A space to add custom button inputs",
        options = c("yes", "no")
      )
    )

show_info(test)
view_trelliscope(test) 


# here("C:\\Users", "spd19", "AppData", "Local", "Temp", "Rtmpsf3mnt", "file65f4368151a3", "index.html") |> shell.exec()
```
```{r}
test2 <- 
  mars_rover %>%
  slice_sample(prop = 0.01) %>%
  rename("rando" = img_src) %>%
  as_trelliscope_df(name = "test2")

show_info(test2)
```


```{r}
# Use `as_trelliscope_df()` to convert panel metadata to a special
# trelliscope data frame
library(ggplot2)
library(dplyr)

panel_dat <-
  (
  ggplot(gap, aes(year, life_exp)) +
    geom_point() +
    facet_panels(vars(country, continent))
  ) |>
    as_panels_df()

meta_dat <- 
  summarise(
    gap,
    .by = c(country, continent),
    mean_life_exp = mean(life_exp),
    min_life_exp = min(life_exp),
    max_life_exp = max(life_exp),
    mean_gdp = mean(gdp_percap)
  )

joined_dat <- 
  left_join(panel_dat, meta_dat) |>
  as_trelliscope_df(name = "life_expectancy", path = tempfile())

if (FALSE) {
view_trelliscope(joined_dat)
}


trell <- 
  joined_dat |>
# as_trelliscope_df(name = "Select Images") |>
  add_inputs(
    # input_radio(
    #   name = "Radio Input",
    #   label = "A space to add custom ranking for sorting",
    #   options = c("yes", "no")
    # ),
    input_select(
      name = "Select Input",
      label = "A space to add custom dropdown inputs",
      options = c("yes", "no"),
      active = TRUE
    ),
    # input_checkbox(
    #   name = "Checkbox Input",
    #   label = "A space to add custom button inputs",
    #   options = c("yes", "no")
    # ),
    vars = c("continent", "country")
  )

view_trelliscope(trell)


here("data") %>%
  dir_ls(regexp = "_2trel") %>%
  read_csv() 
```


```{r}
shelf(trelliscope)
# rstudioapi::selectDirectory()
file_dir <- 
  here("C:\\Users", "spd19", "Box", "Davidson_Fellowship_2022", "SAMPLING", 
       "FEBRUARY_2023", "Mairim", "photo") 

save_csv_name <- 
  str_extract(file_dir, ".*((?i)sampling)/(.*)", group = 2) %>%
  str_replace_all("/", "_") %>%
  str_to_lower() %T>% print()  
  
nat_files <-
  file_dir %>%
  dir_ls(regexp = "\\.JPG") %>%
  tibble(src_img = .) %>%
  # slice(1:15) %>%
  mutate(
    .before = 1,
    id      = str_extract(src_img, "GOPR(\\d+)\\.JPG", group = 1)
    ) %>%
  mutate(
    dir_og = dirname(src_img),
    loc  = str_extract(src_img, "SAMPLING/(.*20\\d{2})/(.*)/photo", group = 2),
    time = str_extract(src_img, "SAMPLING/(.*20\\d{2})", group = 1),
    time = str_to_title(time),
    time = str_replace(time, "_", " ")
  )  %T>% print() %>%
  as_trelliscope_df(name = str_c(save_csv_name, "_to_remove")) %>%
  set_default_layout(ncol = 4, page = 2) %>%
  add_inputs(
    input_radio(
      name = "remove_bad_images",
      label = "Yes images will be removed, no imagaes will be saved",
      options = c("yes", "no"),
      active = TRUE
    ),
    vars = c("dir_og", "src_img", "loc", "time")
  )

view_trelliscope(nat_files)

```

```{r}
files_mv <- 
  here("data") %>%
  dir_ls(regexp = "(?i)photo_to_remove") %>%
  str_subset("_1") %>%
  read_csv(show_col_types = FALSE) %>%
  janitor::clean_names() %>%
  mutate(
    # .keep = "used",
    reg = basename(src_img),
    # og_loc2 = dirname(dir_og),
    file_rm = map2_chr( 
      .x = dir_og,
      .y = reg,
      ~ dir_ls(.x, regexp = .y)),
    # new_loc = str_c(dirname(dir_og), "bad", sep = "/"),
    new_loc = str_c(here("data"), "bad", sep = "/")
  ) %T>% print()


dir_create(unique(files_mv$new_loc))

files_mv  %>%
  filter(str_detect(remove_bad_images, "(?i)yes"))  %>%
  nest(.by = c(dir_og, new_loc)) %$%
  walk2(
    .x = new_loc,
    .y = data,
    \(.x, .y) {
     cli::cli_alert_info("Moving files to {.path {.x}}") 
     cli::cat_bullet(basename(.y$file_rm))
     
     if (menu(c("Yes, move files", "No, don't move files")) == 1) {
       cli::cli_alert_info("Moving files!")
       dir_create(unique(.x))
      file_copy(.y$file_rm, .x)
      # file_move(.y$file_rm, .y$new_loc)
       
     }
     
    }
  )


```


```{r}
dir_create(here("data", "bad2"))

here("data", "bad") %>%
  dir_ls() %T>% print() %>% 
  file_move(., here("data", "bad2"))
  
```

# 5.0 ---- Test Formula Notation ----

From [datacamp](https://www.datacamp.com/tutorial/r-formula-tutorial)

Section: `How to Concatenate Formulae`

```{r formula-notation}
i <- "y ~ x"
j <- "y ~ x + x1"
k <- "y ~ x + x1 + x2"

l <- c(i, j, k)

lapply(l, as.formula)
l

class(j)
class(as.formula(j))
class(y ~ x)

identical(as.formula(i), y ~ x)

```

# 6.0 ---- Test model.frame function ----

from [datacamp](https://www.datacamp.com/tutorial/r-formula-tutorial)

Section: `Modeling Functions`

Returns a dataframe that match the formula columns
- this may do some preprocessing of the data like log transformations

Subset can be added to filter the data further

```{r model-frame}
# use model.frame with the iris data set 

iris
model.frame(Sepal.Length ~ Sepal.Width, data = iris)
model.frame(
  Sepal.Length ~ Sepal.Width * Petal.Length, 
  data = iris)
model.frame(
  Sepal.Length ~ log(Sepal.Width) * Petal.Length, 
  data = iris)

model.frame(
  Sepal.Length ~ Sepal.Width * Petal.Length, 
  data = iris,
  subset = Species == "setosa" # extra filtering
  )
```


# 7.0 ---- Test `adespatial` ----

```{r test-adespatial}
vignette("tutorial", package = "adespatial")
```




# 8.0 ---- Test Kernal Density Estimation ----

```{r test-kde}
# equation for kernel density estimation

kernel_test <- function(radius, .dist) {
  
   max(0, radius^2 - .dist^2)
  
    # ---- end of function kernel_test
}

tibble(x = 1:10) %>%
  mutate(
    .dist = 5,
    radius = 2,
    kernel = map2_dbl(radius, .dist, kernel_test)
  ) 

```


# 9.0 ---- Test `openxlsx2` Formula Builder ----

Using `openxslx2::add_formula`
- basically you create an array formula where you use first col first row to
  first column last row to create an array formula
  i.e. "< column X > < row Y > : < column X > < row Z >" 

- then add the dimensions to put the results of the formula
  - same rows as formula but different columns
    i.e. dim = "< column XX > < row Y > : < column XX > < row Z >" 
  - NOTE: its recommended to have the same length (i.e. row 1 - 10 for both 
    formula and result)
    - they don't have to be the same though, but at least the same number of 
      rows (i.e. formula rows A1:A3 with results A4:A6)
    - if not same number of rows, the formula will stop wherever the row number
      stops


```{r xlsx-formula}
shelf(openxlsx2)

set.seed(123)
# 3 x 5 matrix of random numbers from 0 to 1
dat <- matrix(runif(15), nrow = 3, ncol = 5) %T>% print()

# Create artificial xlsx file
wb <- 
  wb_workbook()$
  add_worksheet()$
  add_data(x = dat, col_names = FALSE)
  
# see startind wb
wb_to_df(wb, col_names = FALSE, , show_formula = TRUE)

# add formulas to individual rows
wb <- 
  wb$
  add_formula(x = "SUM(A2, B2)", dims = "F2")$
  add_formula(x = "A1 + B1", dims = "F1")$
  add_formula(x = "A3 * B3", dims = "F3")
  
# these formulas won't be evaluated until saved and opened in excel
wb_to_df(wb, col_names = FALSE, show_formula = TRUE)
wb$open()

# add array formulas
# run the same operations to many rows
# the formula will only show at the first instance of the formula but will be
# evaluated for all rows once saved
wb <- 
  wb_workbook()$
  add_worksheet()$
  add_data(x = dat, col_names = FALSE)$
  add_formula(x = "A1:A3 * B1:B3", 
              dims = "F1:F3",
              # dims = "A4:A5", # can be evaluated anywhere 
              array = TRUE) 
  
wb_to_df(wb, col_names = FALSE, show_formula = TRUE)

wb$open()

unshelf(openxlsx2)
```

# 10.0 ---- Determine absoprtion, scattering and backscattering cross section ----

From [@stramski2001]

$$

\begin{align}
& \text{Note: this is in}~\mu \text{m and need to conver to m by} \times 10^{-12}  \\
& \text{Absorption} \\
& \text{Detritus:}~\sigma_{a,~det} = 8.791 \times 10^{-4} \times e^{(-0.00847~\times~\lambda)} \\
& \text{Minerals:}~\sigma_{a,~min} = 1.013 \times 10^{-3} \times e^{(-0.00846~\times~\lambda)} \\
& \text{Bubbles:}~\sigma_{a,~bub} = 0 \\
\\ 
& \text{Scattering} \\
& \text{Detritus:}~\sigma_{b,~det} = 0.1425 \times \lambda^{-0.9445} \\
& \text{Minerals:}~\sigma_{b,~min} =  0.7712 \times \lambda^{-0.97164} \\
& \text{Bubbles:}~\sigma_{b,~bub} =  4607.873~(\pm5.555) \\
\\ 
& \text{Backcattering} \\
& \text{Detritus:}~\sigma_{bb,~det} = 5.881 \times \lambda^{-0.8997} \\
& \text{Minerals:}~\sigma_{bb,~min} = 1.790 \times 10^{-2} \times \lambda^{-0.9140} \\
& \text{Bubbles:}~\sigma_{bb,~bub} =  55.359~(\pm0.373) \\
\end{align}

$$

```{r stramski-test}
func_list <- list(
  
  # ---- Absorption ---- #
  # detritus
  a_det = function(lambda, part_conc) {
    8.791e-4 * exp(-0.00847 * lambda) * part_conc * 1e-12
  },
  # minerals
  a_min = function(lambda, part_conc) {
    1.013e-3 * exp(-0.00846 * lambda) * part_conc * 1e-12
  },
  # bubbles
  a_bub = 0,
  
  # ---- Scattering ---- #
  # detritus
  b_det = function(lambda, part_conc) {
    0.1425 * (lambda ^ -0.9445) * part_conc * 1e-12
  },
  # minerals
  b_min = function(lambda, part_conc) {
    0.7712 * (lambda ^ -0.97164) * part_conc * 1e-12
  },
  # bubbles
  b_bub = function(part_conc) {
    4607.873 * part_conc * 1e-12
  },
  
  # ---- Backscattering ---- #
  # detritus
  bb_det = function(lambda, part_conc) {
    5.881 * (lambda ^ -0.8997) * part_conc * 1e-12
  },
  # minerals
  bb_min = function(lambda, part_conc) {
    1.790e-2 * (lambda ^ -0.9140) * part_conc * 1e-12
  },
  # bubbles
  bb_bub = function(part_conc) {
    55.359 * part_conc * 1e-12
  }
)


# ---- Dataframe ---- #
spectral_data <- 
  tibble(
    # wavelength
    lambda = seq(350, 750, 1),
    
    # absorption
    a_det = func_list$a_det(lambda, 8.25e13),
    a_min = func_list$a_min(lambda, 2.75e13),
    a_bub = func_list$a_bub,
    
    # scattering
    b_det = func_list$b_det(lambda, 8.25e13),
    b_min = func_list$b_min(lambda, 2.75e13),
    b_bub = func_list$b_bub(1.775e6),
    
    # backscattering
    bb_det = func_list$bb_det(lambda, 8.25e13),
    bb_min = func_list$bb_min(lambda, 2.75e13),
    bb_bub = func_list$bb_bub(1.775e6),
    
    # backscattering ratio
    bb_b_det = bb_det / b_det,
    bb_b_min = bb_min / b_min,
    bb_b_bub = bb_bub / b_bub
  ) %T>% 
  print()

# ---- Plot ---- #
# Elements
func_list$plt_elements <- list(
  labs(
    x = "Wavelength (nm)",
    color = "Components"
  ),
  # facet_wrap(~ component, scales = "free_y"),
  scale_color_manual(
    labels = c("Bubbles", "Detritus", "Minerals"),
    values = c("red", "blue", "green")
    ),
  coord_cartesian(ylim = c(0, NA)),
  theme_bw()
)


# Absorption
spectral_data %>%
  pivot_longer(
    cols = starts_with("a_"),
    names_to = "component",
    values_to = "absorption"
  ) %>%
  ggplot(aes(x = lambda, y = absorption)) +
  geom_line(aes(color = component)) +
  labs(y = "Absorption (m^-1)") +
  func_list$plt_elements

# Scattering
spectral_data %>%
  pivot_longer(
    cols = starts_with("b_"),
    names_to = "component",
    values_to = "scattering"
  ) %>%
  ggplot(aes(x = lambda, y = scattering)) +
  geom_line(aes(color = component)) +
  labs(y = "Scattering (m^-1)") +
  func_list$plt_elements

# Backscattering
spectral_data %>%
  pivot_longer(
    cols = matches("bb_(bub|min|det)"),
    names_to = "component",
    values_to = "backscattering"
  ) %>%
  ggplot(aes(x = lambda, y = backscattering)) +
  geom_line(aes(color = component)) +
  labs(y = "Backscattering (m^-1)") +
  func_list$plt_elements

# Backscattering ratio
spectral_data %>%
  pivot_longer(
    cols = starts_with("bb_b_"),
    names_to = "component",
    values_to = "backscattering_ratio"
  ) %>%
  ggplot(aes(x = lambda, y = backscattering_ratio)) +
  geom_line(aes(color = component)) +
  labs(y = "Backscattering ratio") +
  func_list$plt_elements

```


# 11. ---- Pressure vs Depth Calculations ----

This was an interesting calculatino for the difference of pressure (dB) to real
depth (m). 

The calculation:
[Practical Conversion of Pressure to Depth](https://journals.ametsoc.org/view/journals/phoc/11/4/1520-0485_1981_011_0573_pcoptd_2_0_co_2.xml)


$$ 

\begin{align}

& z = (1 - c_1) \times p - (c_2 \times p^2)  \\
& c_1 = (5.92 + 5.25 \times \sin^2\phi) \times 10^{-3} \\
&  c_2 = 2.21 \times 10^{-6} \\
& z = \text{depth in meters}~(m) \\
& \phi = \text{latidude in degrees}~(^{\circ}) \\

\end{align}

$$

```{r pressure-depth-calc, fig.width=20, fig.asp=0.5}
if (!nzchar(system.file(package = "librarian"))) 
  install.packages("librarian")

librarian::shelf(
  quiet = TRUE,
  librarian, conflicted, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
  forcats, lubridate, glue, fs, magrittr, here,
  
  # additional
  
)

# shelf(conflicted) # may be needed if won't allow loading of certain packages

conflicts_prefer(
  dplyr::filter(), 
  dplyr::select()
  )


# pressure from 0 to 500 dB
pres <- seq(0, 1000, 0.5)

# latidude from 0 to 90
phi  <- rep(0:90, each = length(pres))
phi  <- seq(0, 90, by = 10)

depth_data <- 
  # match all phi with all pres
  expand.grid(phi = phi, pres = pres) %>%
  mutate(
    
    # degrees
    phi2 = phi,
    
    # radians
    phi = phi * pi / 180,
    
    # calculate c1
    c1 = map_dbl(
      phi,
      \(phi) (5.92 + 5.25 * sin(phi)^2) * 10^-3
    ),
    
    # calculate depth
    z = map2_dbl(
      pres, 
      c1,
      \(p, c1) (1 - c1) * p - c2 * p^2),
    
    # calculate difference in depth and pressure
    diff = z - pres,
    row = row_number()
    ) %T>% print() 

# line plot z and phi 
(
  depth_data %>%
  ggplot(aes(x = row, group = phi)) +
  geom_line(aes(y = pres), color = "red", alpha = 0.5) +
  geom_line(aes(y = z), alpha = 0.5) +
  scale_y_reverse() + 
  facet_wrap(~phi2, ncol = 3)
   
  
 ) %>%
  plotly::ggplotly()

# scatter plot z vs phi 
(
  depth_data %>%
  ggplot(aes(x = pres, y = z, group = phi)) +
  geom_abline(slope = 1, color = "red") +
  geom_point(size = 0.25, alpha = 0.5) +
  labs(
    x = "Pressure (dB)",
    y = "Depth (m)"
  ) +
  facet_wrap(~phi2, ncol = 3) +
  coord_cartesian(xlim = c(0, NA), ylim = c(0, NA), expand = FALSE)
   
  
 ) %>%
  plotly::ggplotly()

# scatter plot of z - phi
(
  depth_data %>%
  ggplot(aes(x = pres, y = diff, group = phi)) +
  geom_abline(slope = 0, color = "red") +
  geom_point(size = 0.25, alpha = 0.5) +
  labs(
    x = "Pressure (dB)",
    y = "Difference"
  ) +
  facet_wrap(~phi2, ncol = 3) 
   
  
 ) %>%
  plotly::ggplotly()
```

# 12. ---- Use `do.call` to create variables after function ----

NOTE: moved to quick_tip as `Export Variables from Function` with more 
      functionality, explanation, and examples

```{r export-var-list}


create_vars_from_fun <- function(x) {
  x_name <- (deparse(substitute(x)))
  x2 <- paste(x_name, names(x), sep = "_")
  print(x2)
  x <- setNames(x, x2)
  
  for (i in x2) {
   do.call("<<-", list(i, x[[i]]))
    
    print(ls())
  }
  
}

# example
# linear model using mtcars
mod1 <- lm(mpg ~ wt, data = mtcars)
names(mod1)
create_vars_from_fun(mod1)


  
```
# Add Cycle Wrapper and Idea About Indexing Multiple Columns for Each Loop




```{r cycle-ideas}
# helps with wrapping around numbers, so here, add a shift number to a number 
# but if exceeds the max number, goes back to 1. this will ignore 0 as an option
# here 1:12 = month numbers
# add 6 months
# once exceeds 12, goes back to 1
# issues: can't easily change the starting number to be something different
map_int(
    1:12,
    \(x, shift = 6, max_n = 12, start_value = 0) {
      ((x + shift - 1) %% max_n) + 1
      # is_max <- TRUE
      # while (is_max) {
      #   x <- ((x + shift - start_value - 1) %% max_n) + start_value + 1 # shifts number by x, then subtract 1
      #   is_max <- x > max_n
      # }
      # return(x)
    }
  ) %>%
  bind_cols(month = month.name, old_m = x, new_m = .)

# alternate function factory: set the number to wrap around (n)
# wraps around at 1
cycle_val <- function(n) {
  function(x) (x - 1) %% n + 1
}

# set number
month_cycle <- cycle_val(12)
month_cycle(1:12 + 6)


# interesting code idea
# x[, (m <- m + 1)] adds a counter to the column to increment the column
# twice per loop for cosx and sinx
fHarmonic <- function(theta, k = 4) {
  # browser()
  x <- matrix(0, length(theta), 2 * k)
  nam <- as.vector(outer(c("c", "s"), 1:k, paste, sep = ""))
  dimnames(x) <- list(names(theta), nam)
  m <- 0 # starts at 0
  for (j in 1:k) {
    x[, (m <- m + 1)] <- cos(j * theta) # m increments by one, so column 1
    x[, (m <- m + 1)] <- sin(j * theta) # m increments by one, so column 2
    # the next loop would start on column 3, i.e. m = 3
  }
  x
}

fHarmonic(c(10, 1, 12, 12))
```



# Read png Files and Extract RGB Values

png files are typically stored as:
- RGB (3 channels)
- RGBA (4 channels)
- Grayscale (1 channel)
- GA (2 channels)

NOTE: A is for alpha

```{r png-load}
# image from package test
test_img <- png::readPNG(system.file("img", "Rlogo.png", package = "png"), 
                         info = TRUE)


# extract each channel (RGB) and multiply to convert decimal to 0 - 255
red   <- test_img[,,1] * 255 
green <- test_img[,,2] * 255
blue  <- test_img[,,3] * 255
alpha <- test_img[,,4]

mean(red) 
mean(green)
mean(blue)


# test image
test_img2 <- 
  here("data", "raw") %>%
  dir_ls(regexp = "test_img") %>%
  png::readPNG(info = TRUE)

str(test_img2)

# extract each channel (RGB) and multiply to convert decimal to 0 - 255
red   <- test_img2[,,1] * 255 
green <- test_img2[,,2] * 255
blue  <- test_img2[,,3] * 255


mean(red) 
mean(green)
mean(blue)


# test image
test_img2 <- 
  here("data", "raw") %>%
  dir_ls(regexp = "scope") %>%
  jpeg::readJPEG()

str(test_img2)

# extract each channel (RGB) and multiply to convert decimal to 0 - 255
red   <- test_img2[,,1] * 255 
green <- test_img2[,,2] * 255
blue  <- test_img2[,,3] * 255

rm(test_img2)

mean(red) 
mean(green)
mean(blue)

mean(green) / mean(blue)
mean(green) / mean(red)
```


```{r planktoscope-wb-cal}
# planktoscope image
wb_cal <- function(file, red_wb_og = 2.4,  blue_wb_og = 1.35) {

  test_img2 <- 
    # rstudioapi::selectFile() %>%
    jpeg::readJPEG(file)
  
  
  # extract each channel (RGB) and multiply to convert decimal to 0 - 255
  red   <- test_img2[,,1] * 255 
  green <- test_img2[,,2] * 255
  blue  <- test_img2[,,3] * 255
  
 
  iso <- if_else(str_detect(file, "ISO"),
          str_extract(file, "ISO_\\d{2,4}"),
          NA)
    
  
  dat <- data.frame(
    file       = basename(file),
    iso        = iso,
    red_wb_og  = red_wb_og,
    blue_wb_og = blue_wb_og,
    r_avg      = mean(red), 
    g_avg      = mean(green),
    b_avg      = mean(blue),
    new_red    = mean(green) / mean(red) * red_wb_og,
    new_blue   = mean(green) / mean(blue) * blue_wb_og
  )
  
  
  return(dat)
}
file_path <-
  rstudioapi::selectDirectory() %>%
  dir_ls(regexp = "\\.jpg", recurse = TRUE) %T>% 
  print()
  
dat_wb <- 
  file_path %>%
    map(
       ~ wb_cal(.x)
    ) %>%
  list_rbind() %T>% 
  print()


if (FALSE) {
  writexl::write_xlsx(
    dat_wb,
    here(dirname(file_path[1]), glue("white_balance_cal_{Sys.Date()}.xlsx")),
    format_headers = FALSE
  )
}
# shell.exec(here(dirname(file_path[1])))

```

original white balance:
- R: 2.4
- B: 1.35

mean:
- R: 235.5043
- G: 216.2474
- B: 221.8592

- G/R: 0.9182316
- G/B: 0.9747056

- G/R * original: 2.203756
- G/B * original: 1.315853


```{r}
# shelf(phytoclass)
# Sm
# Fm
# simulated_annealing(Sm, Fm, niter = 10)
bench <- microbenchmark::microbenchmark(
    times = 10,
    control = list(order = "inorder"),
    # check = "equal", # uncomment for equal
    og = {
      simulated_annealing(Sm, Fm, niter = 10, verbose = FALSE)
      }
    )
bench
boxplot(bench)

```


# `hasArg`: Check if a non-standard Input Variable is Enter

This is one way to check if an arguemnt is supplied in the `...` where one is
explicitly expected but not always there.

```{r}
myFun <- function(x, y, ...) {
  if (hasArg(z)) {
    print("Yes, 'z' exists")
  } else {
    print("No, 'z' does not exist")
  }
}

myFun(x = 3, z = NULL)
myFun(x = 3)

```

# `cli_process_xx` Test for Message for start, done, and failed!

```{r}
cli_test <- function() {
  cli::cli_process_start(
    "Starting process", 
    msg_done = "I'm finished!",
    msg_failed = "Oops, no I failed!")
  
  Sys.sleep(2)
  
  if (sample(c(0, 1), 1) == 1) {
    cli::cli_process_done()
  } else {
    cli::cli_process_failed()
  }
  
    # ---- end of function cli_test
}

# failed
set.seed(1)
cli_test()

# success
set.seed(4)
cli_test()

```



# Open Arc GIS `.lpk` File
Help: <https://gis.stackexchange.com/questions/34310/opening-lyr-file-via-rgdal-ogr>

```{r}
shelf(sf)

dir_path <- rstudioapi::selectDirectory()

dir_path <-
  here( "E:/", "Final_UnifiedReefMap_Version2.2", 
        "FWC_UnifiedFloridaReefMap_v2.2.gdb")  %T>% 
  print() 

layers <-
  st_layers(dir_path) %T>% 
  print()

test_gdb <-
  dir_path[1] %>%
  sf::read_sf()

test_gdb2 <-
  dir_path %>%
  sf::st_read(layer = "UnifiedReefMap_Archive_v20")

test_gdb2 
test_gdb2$GeoformDet


ggplot(test_gdb2) +
  geom_sf(aes(fill = GeoformDet))

```


# Open Various Files from ERDDAP

<https://gcoos5.geos.tamu.edu/erddap/>

Search:
Walton Smith, max time =
<https://gcoos5.geos.tamu.edu/erddap/search/advanced.html?page=1&itemsPerPage=1000&searchFor=walton+smith&protocol=%28ANY%29&cdm_data_type=%28ANY%29&institution=%28ANY%29&ioos_category=%28ANY%29&keywords=%28ANY%29&long_name=%28ANY%29&standard_name=%28ANY%29&variableName=%28ANY%29&maxLat=&minLon=&maxLon=&minLat=&minTime=2016-01-01T01%3A00%3A00Z&maxTime=>

```{r}
data_path <- here("data", "raw", "erddap") %>%
  dir_ls() %T>% 
  print()

data_path[2] %>%
  # read_csv(n_max = 50)
  read_lines(n_max = 50)
data_path[3] %>%
  # read_csv(n_max = 50)
  read_lines(n_max = 50)

```


# THREDDS HYCOM-TSIS Test

Info: <https://www.hycom.org/data/gomb0pt01/gom-reanalysis>
From: <https://tds.hycom.org/thredds/catalog/datasets/GOMb0.01/reanalysis/data/catalog.html>

Help: <https://docs.unidata.ucar.edu/tds/current/userguide/ncss_grid.html>
- dateTime has the form: '-'? yyyy '-' mm '-' dd 'T' hh ':' mm ':' ss ('.' s+)? (zzzzzz)?

Base URL:
https://ncss.hycom.org/thredds/ncss/datasets/GOMb0.01/reanalysis/data/YYYY/xxx_archv.YYYY_DDD_HH_NN.nc

Experiment Numbers:
This part `xxx_archv.YYYY_DDD_HH_NN.nc` is described here
xxx: experiment number (i.e. 010, 023 or 026 for date ranges)
YYYY: year
DDD: day (julian day)
HH: hour
NN: Netcdf type (i.e. 2d (surface) or 3z (with depth))

HYCOM-TSIS GOMb0.01:
010_archv.YYYY_DDD_HH_NN.nc: 2001_001_00 to 2017_152_18
023_archv.YYYY_DDD_HH_NN.nc: 2017_152_19 to 2024_032_18
026_archv.YYYY_DDD_HH_NN.nc: 2024_001_19 to *PRESENT*


Parameters used in link for 3z:
- variables
  - var=salinity&
  - var=u&
  - var=v&
  - var=w_velocity&
  - var=water_temp
- lat/lon
  - north=26.4375&
  - west=-83.8125&
  - east=-78.3125&
  - south=22.8125
- horizStride=1
  - number of data points to skip along horiztonal (x and y)
- vertStride=1
  - number of data points to skip along vertical (z)
- time=2017-01-02T10%3A01%3A52.500Z
  - yyyy-mm-ddThh:mm::ss
  - %3A is : (colon in URL encoding)
- vertCoord=0.0
  - single level 0.0, 2.0, 4.0, ..., 5000 m

- accept=netcdf4
  - type of netcdf (netcdf or netcdf4)

```{r}
dwnld_path <- here("data", "raw", "hycom")
dir_create(dwnld_path)

base_url <- paste0(
  "https://ncss.hycom.org/thredds/ncss/datasets/GOMb0.01/reanalysis/data/{year}/",
  "{experiment}_archv.{year}_{julian}_{hh}_3z.nc?",
  "var=salinity&var=u&var=v&var=w_velocity&var=water_temp&",
  "north={ymax}&west={xmin}&east={xmax}&south={ymin}&",
  # "disableLLSubset=on&",
  "disableProjSubset=on&",
  "horizStride={horizon}&",
  "time={year}-{mm}-{dd}T{hh}%3A{mmin}%3A{ss}Z&", 
  "vertCoord={vert}&accept=netcdf4"
  )

experiment <- "010"
year       <- "2017"
julian     <- "003"
hh         <- "12"
mm         <- "01"
dd         <- "03"
hh         <- "12"
mmin       <- "00" 
ss         <- "00"
vert       <- "0.0"
horizon    <- 5 

xmin <- -83.8125
xmax <- -78.3125
ymin <- 22.8125
ymax <- 26.4375

# xmin <- -83
# xmax <- -82
# ymin <- 22
# ymax <- 23

base_url
glue(base_url) %>%
  download.file(destfile = here(dwnld_path, glue("hycom_{year}_{julian}.nc4")),
  method = "curl"
  )


download.file(
  url = "https://ncss.hycom.org/thredds/ncss/datasets/GOMb0.01/reanalysis/data/2017/010_archv.2017_003_12_3z.nc?var=salinity&var=u&var=v&var=w_velocity&var=water_temp&north=23&west=-90.5&east=-89&south=22.5&horizStride=1&time=2017-01-03T12%3A00%3A00Z&vertCoord=0&accept=netcdf4",
  destfile = here(dwnld_path, "test5.nc4"),
  method = "curl"
  )

options(timeout = max(5000, getOption("timeout")))
# options(timeout = 60)

```

https://ncss.hycom.org/thredds/ncss/datasets/GOMb0.01/reanalysis/data/2017/
https://ncss.hycom.org/thredds/ncss/datasets/GOMb0.01/reanalysis/data/2017/

010_archv.2017_003_12_3z.nc?var=salinity&var=u&var=v&var=w_velocity&
010_archv.2017_003_12_3z.nc?var=salinity&var=u&var=v&var=w_velocity&

var=water_temp&north=26.4375&west=-83.8125&east=-78.3125&south=22.8125
var=water_temp&north=31.9   &west=-98.0000&east=-77.0000&south=20&

disableLLSubset=on&disableProjSubset=on&horizStride=1&time=2017-01-03T
disableProjSubset=on&horizStride=1&time=2017-01-03T

12%3A00%3A00Z&vertCoord=0.0&accept=netcdf4
12%3A00%3A00Z&vertCoord=0.0&accept=netcdf4






